# Project 3 - W205 - Fundamentals of Data Engineering
### UC Berkeley - MIDS
### Marcelo Scatolin Queiroz


#### Note for students using this repo:
Here are the files of my work during my Data engineering class for MIDS. I published them here as a way to encourage self-development and learning. Please, do not copy this files as a full answer to your assignments. This is against Berkeley's code of conduct, highly unethical and the worst, you will not learn as intended.

Feel free to reach out to me with questions, suggestions and critics.


## Introduction

This is a project that intents to cover the concepts of streaming data from a web server as a simulated API running on Flask to a Kafka service. After that we want to be able to analyze the data using different tools, and each assignment will be build on the previous one, adding complexity and functionality to this project.

The scenario is set with the following assumptions:
- I'm a data scientist at a game development company.  
- Your latest mobile game has two events you're interested in tracking:
  - `buy a sword` & `join guild`;
  - Each has metadata.

## Executive Summary:

After preparing the files needed (a `.yml` file for configuring the cluster and a `.py` for simulating the API), we used a Hadoop cluster running Flask and Kafka in different droplets to respectively generate and consume events based on web protocol. Additionally, a container running spark was used to manipulate the json logs generated by our flask application and recorded a Kafka topic. Every time a client used one of the designated URLs, the flask application returns a message and send an event to a Kafka topic with json schema. The pySpark interface is then able to read it, perform queries and analyze the results. Additionally we added `.py` files to do events extraction, transform and filter events in batches, writing the outputs in our Hadoop cluster and enabling further analysis. In the end, the final user will be able to use dictionaries objects to analyze the player behavior, like in a real game API.

## Architecture

I created 5 containers in a Digital Ocean Droplet machine using Ubuntu distribution. Each container will run a different service needed for our analysis. The image bellow summarize each one and their use. Further details and specific of how each container passes data to the other can be found in the `docker-compose.yml` file that can be found next.

![Simplified Architechture](Hadoop_Cluster_schematics.png "Figure 01")

```{yml}
---
version: '2'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 32181
      ZOOKEEPER_TICK_TIME: 2000
    expose:
      - "2181"
      - "2888"
      - "32181"
      - "3888"
    extra_hosts:
      - "moby:127.0.0.1"

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    expose:
      - "9092"
      - "29092"
    extra_hosts:
      - "moby:127.0.0.1"

  cloudera:
    image: midsw205/cdh-minimal:latest
    expose:
      - "8020" # nn
      - "50070" # nn http
      - "8888" # hue
    #ports:
    #- "8888:8888"
    extra_hosts:
      - "moby:127.0.0.1"

  spark:
    image: midsw205/spark-python:0.0.5
    stdin_open: true
    tty: true
    expose:
      - "8888"
    ports:
      - "8888:8888"
    volumes:
      - "home/science/w205:/w205"
    command: bash
    depends_on:
      - cloudera
    environment:
      HADOOP_NAMENODE: cloudera
    extra_hosts:
      - "moby:127.0.0.1"

  mids:
    image: midsw205/base:latest
    stdin_open: true
    tty: true
    expose:
      - "5000"
    ports:
      - "5000:5000"
    volumes:
      - "/home/science/w205:/w205"
    extra_hosts:
      - "moby:127.0.0.1"

   ```

## The application

In order to simulate a web application (or the game, in this case) I used Flask, a framework for web applications written in Python which is simple and lightweight. The application has one `game_api.py` program which implements a class called User and interacts with the Flask framework, generating responses in our localhost when the correct URLs are asked. Additionally, I use the useful Flask/Kafka integration, using the later to produce messages in topic called 'events' and being able to consume this messages later. To simulate an production environment better I also used Apache Bench to generate multiple events at a time.

The idea of the class was to simulate one application and it has 4 simple methods: the first is a simple constructor, the second returns an user inventory, the third adds an object to an inventory (usually a string) and the last one updates or create a guild name for the user. No further develops on checking user inputs and outputs were done, as the scope of this project is the data stream, not the application itself.

Bellow, you will find the `game_api.py` source file commented:

```{python}
import json
from kafka import KafkaProducer
from flask import Flask

#The Flask object will be used to create the decorators that calls functions for event generation.
app = Flask(__name__)

#The KafkaProducer object will login to Kafka and post the messages in the queue
producer = KafkaProducer(bootstrap_servers='kafka:29092')

def log_to_kafka(topic, event):
    producer.send(topic, json.dumps(event).encode())


class User:
""" Class user contains the attributes:
    * name: string
    * inventory: list
    * guild: string

    Represents a player and stores its items and to what guild it belongs.
"""
    def __init__(self, name):
        self.name = name
        self.inventory = []
        self.guild = ''

    def get_inventory(self):
    """returns inventory as a list"""
        return (self.inventory)

    def add_to_inventory(self, item):
    """ Add an item to class User object and returns a confirmation string
        syntax: add_to_inventory(User, string)"""
        self.inventory.append(item)
        return (item + ' added to '+ self.name + ' inventory')

    def change_guild (self, guild):
    """Check for existing guild for object User, updates with guild
    and returns confirmation string
    syntax: change_guild(User, string)"""
        if self.guild == '':
            self.guild = guild
            return (self.name + ' joined the ' + self.guild + ' Guild')
        else:
            aux = self.guild
            self.guild = guild
            return (self.name + ' left the ' + aux + ' Guild and joined the '
                    + self.guild + ' Guild')


#decorator maker for default_response function.
@app.route("/")
def default_response():
""" Test function for Flask web app. Returns default string"""
    default_event = {'event_type':'default'}
    log_to_kafka('events', default_event)
    return "\nTest Succeed. This is the default response!\n"


@app.route("/purchase_a_sword/<username>")
def purchase_a_sword(username):
""" Function creates a User object and add a sword object to its inventory
    it also generates an event to Kakfa MQ and returns a confirmation string"""
    user = User(username)
    purchase_sword_event = {'event_type': 'purchase_sword',
                            'username': user.name}
    log_to_kafka('events', purchase_sword_event)
    return '\n' + user.add_to_inventory('sword') + '\n'

@app.route("/join_guild/<username>/<guild_name>")
def join_guild(guild_name, username):
""" Function creates a User object and add it to a guild.
    It also generates an event to Kafka MQ and returns a confirmation string"""
    user = User(username)
    join_guild_event = {'event_type': 'join guild',
                            'username': user.name,
                            'new_guild': guild_name}
    log_to_kafka('events', join_guild_event)
    return '\n' + user.change_guild(guild_name) + '\n'

```



The second part holds the api itself which uses the class User and defines 3 simple functions that can be recorded using this API:

* `default_response` is a test, returning a default response string.
* `purchase_sword` add a string 'sword' to the user's inventory (creating an User instance if none is there yet).
* `join_guild` changes or add a name of a guild to the User instance.

Note that the last two functions also passes variables with the calling URL. This matter will be addressed later.

## Batch Python Routines

In order to add routines that can make the final user's life easier, we implemented a routine to run on our Spark cluster, reading the events and saving them on our HDFS. This is the `filtered_writes.py` file commented:

```{python}
import json
from pyspark.sql import SparkSession, Row
from pyspark.sql.functions import udf


@udf('boolean')
def is_purchase(event_as_json):
"""function that takes a json formated file, filter keys 'event_type' and
returns True if its values matches 'purchase_sword', otherwise returns false"""
    event = json.loads(event_as_json)
    if event['event_type'] == 'purchase_sword':
        return True
    return False


def main():

    #open spark session
    spark = SparkSession \
        .builder \
        .appName("ExtractEventsJob") \
        .getOrCreate()

    #subscribe to Kafka topic and retrieves all events to raw_events string
    raw_events = spark \
        .read \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "kafka:29092") \
        .option("subscribe", "events") \
        .option("startingOffsets", "earliest") \
        .option("endingOffsets", "latest") \
        .load()

    #apply is_purchase function to filter purchase_sword events
    purchase_events = raw_events \
        .select(raw_events.value.cast('string').alias('raw'),
                raw_events.timestamp.cast('string')) \
        .filter(is_purchase('raw'))

    #save filtered events to a dataframe and shows the schema and the table
    extracted_purchase_events = purchase_events \
        .rdd \
        .map(lambda r: Row(timestamp=r.timestamp, **json.loads(r.raw))) \
        .toDF()
    extracted_purchase_events.printSchema()
    extracted_purchase_events.show()

    #write the selected purchase events to parquet file
    extracted_purchase_events \
        .write \
        .mode('overwrite') \
        .parquet('/tmp/purchases')

if __name__ == "__main__":
    main()

 ```



With those events configured, we started our tests and analysis of some events generated as dummy ones.

## Work Structure

For class grading purposes, the commands ran against each of my docker containers were transcripted here.

We used four terminal windows for this assignment, one running the Flask application, to be able to see the logs, and a second one executing url requests from the mids container, in order to generate the events, the third one for running the kafkacat commands and monitor the topics and the fourth and final one for pySpark inrface window. Unless if express stated, the commands refer to the first window. The comands explanation will be pasted here and explained as follows:

#### Step explanation
```
command used
```
Additional explanation (if necessary)

## Step by Step solution

#### Change the directory
```
cd w205/full-stack
```
As this directory was created in a previous section (sync class), we just wanna make surre all files are well configured.

#### Checking the yml config file for the cluster:
```
vim docker-compose.yml
```

#### Spin up the cluster
```
docker-compose up -d
```
The `docker-compose.yml` file can be found above.

#### Creating the 'events' topic
```
docker-compose exec kafka kafka-topics \
  --create \
  --topic events \
  --partitions 1 \
  --replication-factor 1  \
  --if-not-exists \
  --zookeeper zookeeper:32181
```
The messages will be produced later by the flask app.

#### Copying the game_api.py file from last week's directory:
```
cp ../assignment-11-MScatolin/game_api.py ./game_api.py
```

#### Editing the game_api.py file:
```
vim game_api.py
```
 The file can be found above.

#### Starting up the Flask app
```
docker-compose exec mids \
  env FLASK_APP=/w205/full-stack/game_api.py \
  flask run --host 0.0.0.0
```
We run the environment to make sure we could read the messages generated. In this case the output was:
```
* Serving Flask app "game_api"
* Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
```

#### we will run Kafka in continuous mode, to see the outputs changing live (window 2)
```
docker-compose exec mids kafkacat -C -b kafka:29092 -t events -o beginning
```

#### testing the default event (window 2)
```
docker-compose exec mids curl http://localhost:5000/
```
Output was, as expected, in window 1 (flask):
```
127.0.0.1 - - [16/Jul/2018 11:30:37] "GET / HTTP/1.1" 200 -
```
And in window 2 (regular terminal):
```
Test Succeed. This is the default response!
```
And in Window 3 (kafka):
```
{"event_type": "default"}
```


#### testing the purchase a sword event (window 2)
```
docker-compose exec mids curl http://localhost:5000/purchase_a_sword/marcelo
```
Window 1 output (flask):
```
127.0.0.1 - - [16/Jul/2018 11:30:42] "GET /purchase_a_sword/marcelo HTTP/1.1" 200 -
```
Window 2 output (regular terminal):
```
sword added to marcelo inventory
```
Window 3 output (kafka):
```
{"username": "marcelo", "event_type": "purchase_sword"}
```

#### Testing the join a Guild event (window 2):
```
docker-compose exec mids curl http://localhost:5000/join_guild/Marcelo/Berkeley
```
output in window 1 (flask):
```
127.0.0.1 - - [16/Jul/2018 11:30:59] "GET /join_guild/Marcelo/Berkeley HTTP/1.1" 200 -
```
output in window 2 (regular terminal):
```
Marcelo joined the Berkeley Guild
```
output in window 3 (Kafka):
```
{"username": "Marcelo", "new_guild": "Berkeley", "event_type": "join guild"}
```

#### Using Apache Bench to generate 100 default events  with a comcast host:
```
docker-compose exec mids \
  ab \
    -n 100 \
    -H "Host: user1.comcast.com" \
    http://localhost:5000/
```
The same outputs were repeated, but now 100 times, with now 101 default events recorded.

#### Using Apache Bench to generate 77 events purchase a sword events for marcelo and with a comcast host:
```
docker-compose exec mids \
  ab \
    -n 77 \
    -H "Host: user1.comcast.com" \
    http://localhost:5000/purchase_a_sword/marcelo
```
The same outputs were repeated, but now 78 times.

#### Using Apache Bench to generate 56 default events with a AT&T host:
```
docker-compose exec mids \
  ab \
    -n 56 \
    -H "Host: user2.att.com" \
    http://localhost:5000/
```
The same outputs were repeated, but now 57 times. Note that now we may have reached 135 events

#### Using Apache Bench to generate 22 events purchase a sword events for marcelo and with a AT&T host:
```
docker-compose exec mids \
  ab \
    -n 22 \
    -H "Host: user2.att.com" \
    http://localhost:5000/purchase_a_sword/marcelo
```
The same outputs were repeated, but now 22 times. Note that we may have reached 100 events

#### Usign Apache Bench to generate 37 events join Berkeley events for marcelo and with a AT&T host:
```
docker-compose exec mids \
  ab \
    -n 37 \
    -H "Host: user2.att.com" \
    http://localhost:5000/join_guild/marcelo/Berkeley
```
The same outputs were repeated, but now 37 times. Note that now we may have reached 38 events

#### Using Apache Bench to generate 38 events join Stanford events for marcelo and with a AT&T host:
```
docker-compose exec mids \
  ab \
    -n 22 \
    -H "Host: user2.att.com" \
    http://localhost:5000/join_guild/marcelo/Stanford
```
The same outputs were repeated, but now 22 times. Note that we may have reached 100 events


#### Running the parquet writing routine (window 2):
```
docker-compose exec spark \
  spark-submit /w205/full-stack/filtered_writes.py
```
The output is extensive. Instead of pasting the output here, I will paste the results on the HDFS folder structure

#### Checking the extracted events file existence (window 3):
```
docker-compose exec cloudera hadoop fs -ls /tmp/
```
The output was:
```
Found 3 items
drwxrwxrwt   - mapred mapred              0 2018-02-06 18:27 /tmp/hadoop-yarn
drwx-wx-wx   - root   supergroup          0 2018-08-06 04:54 /tmp/hive
drwxr-xr-x   - root   supergroup          0 2018-08-06 05:20 /tmp/purchases
```
Indicating the folder was created.

#### Tearing down the cluster:
```
docker-compose down
```

From now on, we can have the `filtered_writes.py` batch routine running every X amount of time loading the `purchase_sword` events into our HDFS. We can deploy this to a regular web server and people interested in analyzing the habits of the users of this game could retrieve close-to-real-time data as needed.

## Conclusion


This project was interesting to show the concepts under the hood of an API and its data pipeline. It is interesting to see how such a simple project can involve such a diverse toolbox. Extrapolating this architecture for more complex applications should be the next step, adding tools like Jupyter Notebook or even GUIs like HUE for improved usability.
